---
timestamp: 'Mon Oct 27 2025 16:49:29 GMT-0400 (Eastern Daylight Time)'
parent: '[[../20251027_164929.33148c8b.md]]'
content_id: 705637722905fd30b2f633a1ec4b7c18c9484d527c049699653e8d9879eb021e
---

# response:

You've hit on a crucial distinction in software testing: **unit testing vs. integration testing**.

The tests for `_getEventsByRecommendationContext` currently use a `MockLLM` instead of the real `GeminiLLM` for several important reasons related to unit testing principles:

1. **Isolation and Determinism (Unit Testing)**:
   * **Purpose**: The primary goal of a *unit test* for `EventConcept` is to verify that the logic *within* the `EventConcept` itself works correctly, independent of external services. This means testing the `createRecommendationPrompt` generation and the `parseAndApplyRecommendations` logic.
   * **Control**: LLMs (even Gemini) can sometimes exhibit non-deterministic behavior, or their output format might subtly change. By mocking the `GeminiLLM`, we gain complete control over its "response." This allows us to precisely test how `EventConcept` handles various scenarios, including valid, malformed, or unexpected JSON outputs from the LLM.
   * **Reproducibility**: Unit tests must be fast and consistently reproducible. Relying on a live external API would introduce network latency, potential rate limits, and external service availability issues, making the tests slow, flaky, and non-deterministic.

2. **Speed and Cost**:
   * Making actual API calls to an LLM takes time and often incurs cost. Running many unit tests frequently (e.g., on every code change in CI/CD) with live LLM calls would be expensive and slow down development feedback loops significantly.

3. **Testing Edge Cases**:
   * It's challenging to force a real LLM to produce specific problematic outputs (like malformed JSON, missing fields, or recommending non-existent events) just to test your parsing and error handling logic. Mocking allows us to simulate these edge cases directly and reliably.

**Why we are NOT using the real `GeminiLLM` in these unit tests:**

If we were to use the actual `GeminiLLM` in these tests, we would be performing an **integration test** rather than a unit test. An integration test would verify:

* That `EventConcept` can successfully connect to and communicate with the `GeminiLLM`.
* That the `GeminiLLM` (given our prompt) provides *meaningful and accurate* recommendations based on its understanding of the data and prompt.

**How "Actual LLM Behavior" (AI Quality) would be tested:**

To verify the "actual LLM behavior" – specifically, the *quality* and *relevance* of the recommendations generated by `GeminiLLM` in response to the `createRecommendationPrompt` – you would typically:

1. **Create Dedicated Integration Tests**: These would be separate, slower tests that *do* instantiate and use the real `GeminiLLM` (likely requiring a valid API key and internet access).
2. **Evaluate LLM Output for Quality**: Instead of simple `assertEquals`, you'd need more sophisticated evaluation. This could involve:
   * **Golden Datasets**: A set of predefined scenarios (user, filters, priorities, candidate events) with human-curated "correct" or "expected" recommendations. The test would compare the LLM's output against this golden set.
   * **Heuristics/Fuzzy Matching**: Since LLM outputs can vary slightly, you might use heuristics to check if the recommendations are "sufficiently close" or "relevant" rather than an exact match.
   * **Human-in-the-Loop Review**: For critical AI features, some level of manual review of LLM outputs might be necessary, especially for non-deterministic results.
   * **Metrics**: Depending on the complexity, you might calculate metrics like precision, recall, or ranking accuracy.

**In summary**: The current tests effectively verify the `EventConcept`'s internal logic for handling LLM responses. They ensure that our parsing is robust and that our filtering mechanisms work correctly, *regardless* of what the LLM generates. Testing the *intelligence* or *accuracy* of the LLM's recommendations is a separate, more complex endeavor that falls under integration or AI quality testing, rather than unit testing the `EventConcept` itself.

The `MockLLM` allows us to isolate the `EventConcept` and verify its logic predictably and efficiently.
